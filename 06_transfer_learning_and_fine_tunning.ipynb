{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465d1aa6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d1a969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4620d7d4",
   "metadata": {},
   "source": [
    "## Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5b0280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb07010",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74ad2cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 3\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 1024\n",
    "num_epochs = 10\n",
    "load_model = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a12afe0",
   "metadata": {},
   "source": [
    "## Load Pretrained model and Modify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a577d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): Identity()\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Pytorch has some inbuilt models which we can use\n",
    "## If you are using same dataset for training then same pretrained weights can be used\n",
    "## but here we are going to use CIFAR10 dataset, so need to modify the model\n",
    "## to match dimensions\n",
    "## We need to modify the avgpool to get output (1,1) and the classifier to get output classes = 10\n",
    "import os\n",
    "\n",
    "# One way to change classifier and avgpool\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "model = torchvision.models.vgg16(pretrained=True)\n",
    "model.avgpool = Identity()     #as want to change complete avgpool, otherwise avgpool[layer] ie, avgpool[1]\n",
    "#model.classifier[0] = nn.Linear(512, 10)   #512 : num_of_inputs of vgg, 10:out_classes of CIFAR\n",
    "##Try\n",
    "# model.classifier = nn.Sequential(nn.Linear(512,100),\n",
    "#                                 nn.Dropout(p=0.5)\n",
    "#                                 nn.Linear(100,10))\n",
    "model.classifier = nn.Linear(512,10)\n",
    "model.to(device)\n",
    "\n",
    "##OR\n",
    "# for i in range(1,7):\n",
    "#     model.classifier[i] = Identity()\n",
    "print(model)\n",
    "\n",
    "\n",
    "# sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899992a8",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6a7ddb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "==Dataset Loaded==\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.CIFAR10(root='dataset/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# test_dataset = datasets.MNIST(root='dataset/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "# test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print('==Dataset Loaded==')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d30cfc",
   "metadata": {},
   "source": [
    "## Create FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06c81359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=10, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(1,1), stride=(1,1), padding=(1,1))\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        self.fc1 = nn.Linear(20*7*7, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "#         print('x shape after pool2: ',x.shape)\n",
    "\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "#         print('x shape after reshape: ',x.shape)\n",
    "#         print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c82228",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check\n",
    "model = CNN(3,10)\n",
    "x = torch.randn(74,3,32,32)\n",
    "print(model(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2263c6",
   "metadata": {},
   "source": [
    "## Initialize the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31ee6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(in_channels=in_channels, num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c58b8bc",
   "metadata": {},
   "source": [
    "## Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cacb1f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7fdd4a",
   "metadata": {},
   "source": [
    "## Save and Load Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f9b521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoints(state, filename='save_checkpoint.pth.tar'):\n",
    "    print(\"==> Saving checkpoints\")\n",
    "    torch.save(state, filename)\n",
    "    \n",
    "def load_checkpoints(checkpoint):\n",
    "    print(\"==> Loading checkpoints\")\n",
    "    model.load_state_dict(checkpoint['model_check_dict'])          #dictionary keyname to save model checkpoint in training network phase\n",
    "    optimizer.load_state_dict(checkpoint['optim_check_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277366dd",
   "metadata": {},
   "source": [
    "## Load Checkpoint file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "339b249e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'save_checkpoint.pth.tar'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_536\\2568785790.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mload_checkpoints\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'save_checkpoint.pth.tar'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'save_checkpoint.pth.tar'"
     ]
    }
   ],
   "source": [
    "if load_model:\n",
    "    load_checkpoints(torch.load('save_checkpoint.pth.tar'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b96857a",
   "metadata": {},
   "source": [
    "## Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d0cd952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving checkpoints\n",
      "==> Saving checkpoints\n",
      "==> Saving checkpoints\n",
      "==> Saving checkpoints\n",
      "==> Saving checkpoints\n",
      "Loss at epoch 9 was 6.99420\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        checkpoint = {'model_check_dict': model.state_dict(), 'optim_check_dict':optimizer.state_dict()}\n",
    "        save_checkpoints(checkpoint)\n",
    "        \n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        \n",
    "    pred = model(data)\n",
    "    loss = criterion(pred, targets)\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "mean_loss = sum(losses)/len(losses)\n",
    "print(f'Loss at epoch {epoch} was {mean_loss:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2f1660",
   "metadata": {},
   "source": [
    "## Check Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f224fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print(\"Checking accuaracy on Training data\")\n",
    "    else:\n",
    "        print(\"Checking accuracy on Test data\")\n",
    "        \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            \n",
    "            pred = model(x)\n",
    "            _, predictions = pred.max(1)\n",
    "            num_correct += (predictions==y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "            \n",
    "            print(f'Got {num_correct}/{num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
    "    \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65733658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuaracy on Training data\n",
      "Got 44/1024 with accuracy 4.30\n",
      "Got 76/2048 with accuracy 3.71\n",
      "Got 117/3072 with accuracy 3.81\n",
      "Got 150/4096 with accuracy 3.66\n",
      "Got 187/5120 with accuracy 3.65\n",
      "Got 230/6144 with accuracy 3.74\n",
      "Got 272/7168 with accuracy 3.79\n",
      "Got 303/8192 with accuracy 3.70\n",
      "Got 338/9216 with accuracy 3.67\n",
      "Got 369/10240 with accuracy 3.60\n",
      "Got 419/11264 with accuracy 3.72\n",
      "Got 459/12288 with accuracy 3.74\n",
      "Got 491/13312 with accuracy 3.69\n",
      "Got 530/14336 with accuracy 3.70\n",
      "Got 572/15360 with accuracy 3.72\n",
      "Got 603/16384 with accuracy 3.68\n",
      "Got 637/17408 with accuracy 3.66\n",
      "Got 674/18432 with accuracy 3.66\n",
      "Got 717/19456 with accuracy 3.69\n",
      "Got 760/20480 with accuracy 3.71\n",
      "Got 808/21504 with accuracy 3.76\n",
      "Got 844/22528 with accuracy 3.75\n",
      "Got 885/23552 with accuracy 3.76\n",
      "Got 925/24576 with accuracy 3.76\n",
      "Got 962/25600 with accuracy 3.76\n",
      "Got 998/26624 with accuracy 3.75\n",
      "Got 1046/27648 with accuracy 3.78\n",
      "Got 1099/28672 with accuracy 3.83\n",
      "Got 1138/29696 with accuracy 3.83\n",
      "Got 1181/30720 with accuracy 3.84\n",
      "Got 1211/31744 with accuracy 3.81\n",
      "Got 1246/32768 with accuracy 3.80\n",
      "Got 1279/33792 with accuracy 3.78\n",
      "Got 1319/34816 with accuracy 3.79\n",
      "Got 1363/35840 with accuracy 3.80\n",
      "Got 1398/36864 with accuracy 3.79\n",
      "Got 1447/37888 with accuracy 3.82\n",
      "Got 1486/38912 with accuracy 3.82\n",
      "Got 1524/39936 with accuracy 3.82\n",
      "Got 1570/40960 with accuracy 3.83\n",
      "Got 1602/41984 with accuracy 3.82\n",
      "Got 1631/43008 with accuracy 3.79\n",
      "Got 1660/44032 with accuracy 3.77\n",
      "Got 1695/45056 with accuracy 3.76\n",
      "Got 1734/46080 with accuracy 3.76\n",
      "Got 1775/47104 with accuracy 3.77\n",
      "Got 1822/48128 with accuracy 3.79\n",
      "Got 1855/49152 with accuracy 3.77\n",
      "Got 1891/50000 with accuracy 3.78\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_536\\122795834.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcheck_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcheck_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "check_accuracy(train_loader, model)\n",
    "check_accuracy(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75db084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
